# ════════════════════════════════════════════════════════════
# LaTeX-OCR Large Architecture Training Configuration
# Unified dataset: ~1M samples (printed + handwritten)
# Optimized for RTX 5090 32GB VRAM (CUDA)
# Model: dim=512, depth=8+8, ~120M params
# ════════════════════════════════════════════════════════════

# GPU / Device
gpu_devices: null
no_cuda: false

# Model Architecture — Large (512-dim, 8-depth)
encoder_structure: hybrid
backbone_layers:
- 2
- 3
- 7
channels: 1
dim: 512
encoder_depth: 8
heads: 8
num_layers: 8
num_tokens: 8000
max_seq_len: 512
patch_size: 16
decoder_args:
  attn_on_attn: false
  cross_attend: true
  ff_glu: true
  rel_pos_bias: false
  use_scalenorm: false

# Image dimensions
max_width: 448
max_height: 192
min_width: 32
min_height: 32
pad: true

# Tokens
pad_token: 0
bos_token: 1
eos_token: 2

# Enhanced Training Configuration — Optimized for RTX 5090 32GB VRAM (CUDA)
epochs: 30
batchsize: 128
micro_batchsize: 32
testbatchsize: 32
lr: 0.0005
optimizer: AdamW
betas:
- 0.9
- 0.999
weight_decay: 0.01
scheduler: CosineAnnealingWarmRestarts
T_0: 1500
T_mult: 2
eta_min: 0.000001
warmup_steps: 1000
label_smoothing: 0.1
use_amp: true
gradient_clip: 1.0
early_stopping_patience: 10

# Dataset paths (update after running prepare_unified_dataset.py)
data: data/unified/train.pkl
valdata: data/unified/val.pkl
tokenizer: data/unified/tokenizer.json
sample_weights: data/unified/sample_weights.json

# Checkpointing
model_path: checkpoints
name: latex_ocr_large
load_chkpt: null
save_freq: 2
sample_freq: 5000

# Evaluation
temperature: 0.2
valbatches: 50
test_samples: 5

# Logging
debug: false
id: null
seed: 42
