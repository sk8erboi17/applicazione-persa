# ════════════════════════════════════════════════════════════
# LaTeX-OCR Medium Architecture Training Configuration
# Unified dataset: ~1M samples (printed + handwritten)
# Cross-platform: macOS (MPS) / Linux (CUDA) / CPU
# Model: dim=384, depth=6+6, ~65M params
# ════════════════════════════════════════════════════════════

# GPU / Device
gpu_devices: null
no_cuda: false

# Model Architecture — Medium (384-dim, 6-depth)
encoder_structure: hybrid
backbone_layers:
- 2
- 3
- 7
channels: 1
dim: 384
encoder_depth: 6
heads: 8
num_layers: 6
num_tokens: 8000
max_seq_len: 512
patch_size: 16
decoder_args:
  attn_on_attn: true
  cross_attend: true
  ff_glu: true
  rel_pos_bias: false
  use_scalenorm: false

# Image dimensions
max_width: 448
max_height: 192
min_width: 32
min_height: 32
pad: true

# Tokens
pad_token: 0
bos_token: 1
eos_token: 2

# Enhanced Training Configuration — Optimized for M4 48GB
epochs: 15
batchsize: 128
micro_batchsize: 32
testbatchsize: 32
lr: 0.0005
optimizer: AdamW
betas:
- 0.9
- 0.999
weight_decay: 0.01
scheduler: CosineAnnealingWarmRestarts
T_0: 3000
T_mult: 2
eta_min: 0.000001
warmup_steps: 1000
label_smoothing: 0.1
use_amp: true
gradient_clip: 1.0
early_stopping_patience: 5

# Dataset paths (update after running prepare_unified_dataset.py)
data: data/unified/train.pkl
valdata: data/unified/val.pkl
tokenizer: data/unified/tokenizer.json
sample_weights: data/unified/sample_weights.json

# Checkpointing
model_path: checkpoints
name: latex_ocr_medium
load_chkpt: null
save_freq: 2
sample_freq: 2000

# Evaluation
temperature: 0.2
valbatches: 100
test_samples: 5

# Logging
debug: false
id: null
seed: 42
