# ════════════════════════════════════════════════════════════
# LaTeX-OCR dim=256 Training Configuration (v2)
# Unified dataset: ~1M samples (printed + handwritten)
# Optimized for RTX 5090 32GB VRAM (CUDA)
# Model: dim=256, depth=4+4, ~14M params
# ════════════════════════════════════════════════════════════

# GPU / Device
gpu_devices: null
no_cuda: false

# Model Architecture — dim=256 (~14M params)
encoder_structure: hybrid
backbone_layers:
- 2
- 3
- 7
channels: 1
dim: 256
encoder_depth: 4
heads: 8
num_layers: 4
num_tokens: 8000
max_seq_len: 512
patch_size: 16
decoder_args:
  attn_on_attn: true
  cross_attend: true
  ff_glu: true
  rel_pos_bias: false
  use_scalenorm: false

# Image dimensions
max_width: 448
max_height: 192
min_width: 32
min_height: 32
pad: true

# Tokens
pad_token: 0
bos_token: 1
eos_token: 2

# Training — Optimized for RTX 5090 32GB VRAM
epochs: 15
batchsize: 256
micro_batchsize: 64
testbatchsize: 64
lr: 0.0003
optimizer: AdamW
betas:
- 0.9
- 0.95
weight_decay: 0.01
scheduler: CosineAnnealingLR
warmup_steps: 5000
eta_min: 0.000001
label_smoothing: 0.1
use_amp: true
gradient_clip: 1.0
early_stopping_patience: 15
early_stopping_warmup_evals: 10
early_stopping_loss_patience: 5

# Dataset paths
data: data/unified/train.pkl
valdata: data/unified/val.pkl
tokenizer: data/unified/tokenizer.json
sample_weights: data/unified/sample_weights.json

# Checkpointing
model_path: checkpoints
name: latex_ocr_v2
load_chkpt: null
save_freq: 2
sample_freq: 2000

# Evaluation
eval_temperature: 0.0
temperature: 0.2
valbatches: 200
ema_alpha: 0.3
test_samples: 5

# Logging
wandb: false
debug: false
id: null
seed: 42
